{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Q_qS4O722wjK7nfR4uZQj14SoYOZfaYx",
      "authorship_tag": "ABX9TyNyREWfV1FfptfSKWELOQsY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidbardoloye/DSC404F/blob/main/CapstoneProj_SidharthaSarat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4Q6enUCQkco",
        "outputId": "7d0789a8-a16f-43be-eb64-f6c04422836e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['10' '1']\n",
            " ['10000' '1']\n",
            " ['1959' '1']\n",
            " ...\n",
            " ['x' '1']\n",
            " ['y' '1']\n",
            " ['yield' '1']]\n"
          ]
        }
      ],
      "source": [
        "# Capstone project - Sidhartha Sarat Bardoloye\n",
        "# Dated: 18th March 2023\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# clean the words in the file \n",
        "def cleanWord(origWord):\n",
        "  #remove punctuations # \n",
        "  origWord=origWord.translate(str.maketrans('', '', string.punctuation))\n",
        "  #convert to lower case \n",
        "  origWord=origWord.lower()\n",
        "  return (origWord)\n",
        "\n",
        "# define a few stop words to be ignored\n",
        "wordsToBeIgnored=['a','an','is','the','of','etc','as','to','on','he','they','she'\n",
        ",'that','but','by','do','for','it','its','his','her','their','not','if','into','in','etc.'\n",
        ",'be','or','and','their','nor','has','had','have','we','us','me']\n",
        "\n",
        "# Read the file in question\n",
        "file=open('capstoneInput.txt')\n",
        "\n",
        "# initialising a np array to hold all words \n",
        "cleanWordList=np.array([])\n",
        "# iterating through the lines in the file \n",
        "for line in file:\n",
        "  line=cleanWord(line)\n",
        "  words=line.split()\n",
        "  # print (words)\n",
        "  # creating a word list as a numpy array\n",
        "  cleanWordList=np.append(cleanWordList,words) \n",
        "\n",
        "# Process the numpy array for rest of the things \n",
        "\n",
        "# create a new array after removing the stop words from cleanWordList\n",
        "cleanWordList_NoStpWrds=np.setdiff1d(cleanWordList,wordsToBeIgnored)\n",
        "totalCleanWords=cleanWordList_NoStpWrds.size # storing the total clean words minus the stop words\n",
        "\n",
        "# creating a pandas dataframe with 3 columns - \"Words\",\"frequency\",\"prob_occ\"(probability of occurence of the word)\n",
        "df_wordfreq=pd.DataFrame(columns=['words','frequency','prob_occ'])\n",
        "\n",
        "uniqueWords,wordCount=np.unique(cleanWordList_NoStpWrds, return_counts=True)\n",
        "#print(np.asarray((uniqueWords, wordCount)).T)\n",
        "\n",
        "  \n"
      ]
    }
  ]
}